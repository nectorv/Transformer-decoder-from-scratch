# Transformer-decoder-from-scratch
# Transformer-decoder-from-scratch
Welcome to my deep learning project! This endeavor revolves around constructing a decoder-based transformer model and training it on the timeless narratives of Jean de la Fontaine's fables. The primary goal of this project is to enhance my understanding of transformer architectures and explore the attainable outcomes using modest computational resources and a limited dataset.

Project Overview:

Model Architecture: The core of this project involves implementing a decoder-based transformer model. This architecture, known for its prowess in natural language processing, will be tailored to decode and generate narratives inspired by Jean de la Fontaine.

Dataset: The training data comprises a curated selection of Jean de la Fontaine's fables. While the dataset is intentionally kept small, it serves as an intriguing canvas to observe the model's learning capabilities and limitations.

Objectives: The primary objectives include gaining insights into transformer model behavior, experimenting with training on a compact machine, and evaluating performance on a constrained dataset. Through this exploration, we aim to draw valuable lessons about the practicalities and potential enhancements of transformer models in a resource-restricted environment.
