{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b656b667",
   "metadata": {},
   "source": [
    "# Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85592aa8",
   "metadata": {},
   "source": [
    "Welcome to the world of narrative transformation! In this project, we dive into the fascinating realm of transformer models, crafting a decoder-based architecture trained on the timeless fables of Jean de la Fontaine.\n",
    "\n",
    "Our mission is simple yet profound: to understand and harness the power of transformers with limited resources. Armed with a small machine and a modest dataset, we aim to explore the potential of these models in the context of natural language generation.\n",
    "\n",
    "By the end of this journey, we'll not only have a trained model capable of weaving narratives but also a richer comprehension of working with constraints in the realm of machine learning. Join us as we unravel the secrets of transformers in the enchanting world of Jean de la Fontaine's fables!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d1539",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fafc823c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fafc823c",
    "outputId": "2e65531f-b9eb-4b92-8173-eb6812cb1cae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\nessi\\anaconda3\\lib\\site-packages (0.1.99)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "import nltk\n",
    "import re\n",
    "from transformers import CamembertModel, CamembertTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import textwrap\n",
    "import sentencepiece\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4370d11c",
   "metadata": {},
   "source": [
    "PyTorch was selected for its integration with NLP libraries, like Hugging Face's Transformers, simplify implementation. This choice optimizes resource utilization on a small machine while aligning with the project's goal of understanding and achieving results with limited data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d210fda6",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057de66a",
   "metadata": {},
   "source": [
    "The backbone of this project consists of a corpus comprising approximately 250 fables sourced directly from Wikipedia. The dataset is written in a 500 KB text file. During preprocessing, all annotations and indentations originating from Wikipedia were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fab577a5",
   "metadata": {
    "id": "fab577a5"
   },
   "outputs": [],
   "source": [
    "with open('fables.txt', 'r', encoding='ISO-8859-1') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = textwrap.dedent(text)\n",
    "text = text.replace('N ', '')\n",
    "text = text.replace('(', '')\n",
    "text = text.replace(')', '')\n",
    "text = text.replace('ſ', '')\n",
    "text = re.sub(r'\\d+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17c71cc8",
   "metadata": {
    "id": "17c71cc8"
   },
   "outputs": [],
   "source": [
    "with open('fables.txt', 'w') as f:\n",
    "    f.write(text)\n",
    "\n",
    "text = text.replace('\\n', '@')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef78bfe",
   "metadata": {},
   "source": [
    "As the Camembert Tokenizern ignores the '\\n' character, it is replaced by '@' and will be converted back to '\\n' after text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67d43f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'Aigle donnait la chasse à Maître Jean Lapin,@Qui droit à son terrier s'enfuyait au plus vite.@Le trou de l'Escarbot se rencontre en chemin :@Je laisse à penser si ce gîte@Etait sûr ; mais où mieux ? Jean Lapin s'y blottit.@L'Aigle fondant sur lui nonobstant cet asile,@L'Escarbot intercède et dit :@Princesse des Oiseaux, il vous est fort facile@D'enlever malgré moi ce pauvre malheureux ;@Mais ne me faites pas cet affront, je vous prie ;@Et,  puisque Jean Lapin vous demande la vie,@C'est mon voisin, c'est mon compère.@L'Oiseau de Jupiter, sans répondre un seul mot,@Choque de l'aile l'Escarbot,@L'étourdit, l'oblige à se taire,@Enlève Jean Lapin. L'Escarbot indigné@Vole au nid de l'Oiseau, fracasse en son absence@Ses ?ufs, ses tendres ?ufs, sa plus douce espérance :@Pas un seul ne fut épargné.@L'Aigle étant de retour et voyant ce ménage,@Remplit le ciel de cris, et, pour comble de rage,@Ne sait sur qui venger le tort qu'elle a souffert.@Elle gémit en vain, sa plainte au vent se perd.@Il \n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278f0ee",
   "metadata": {},
   "source": [
    "# Creation of the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "511e87fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "511e87fe",
    "outputId": "41ae7f58-67ec-4f5e-f2fd-73aec863e555"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "def encode_text_to_ids(text):\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    return token_ids\n",
    "\n",
    "def decode_ids_to_text(token_ids):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    decoded_text = tokenizer.decode(token_ids)\n",
    "    return decoded_text\n",
    "\n",
    "def create_input_output_pairs(data, sequence_length):\n",
    "    pairs = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        x = data[i:i+sequence_length]\n",
    "        y = data[i+1:i+sequence_length+1]\n",
    "        pairs.append((x, y))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a930d7",
   "metadata": {},
   "source": [
    "'encode_text_to_ids' is used to convert a string into a table of token id. the tokenizer used is the CamembertTokenizer which is adapted to the french language. 'decode_ids_to_text' is used to convert back a table of token id into a string. 'create_input_output_pairs' is used to create the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8317f7ac",
   "metadata": {
    "id": "8317f7ac"
   },
   "outputs": [],
   "source": [
    "data = torch.tensor(encode_text_to_ids(text), dtype=torch.long)\n",
    "\n",
    "sequence_length = 32\n",
    "\n",
    "pairs = create_input_output_pairs(data, sequence_length)\n",
    "\n",
    "train_pairs, val_pairs = train_test_split(pairs, test_size=0.01, random_state=42)\n",
    "\n",
    "train_x, train_y = zip(*train_pairs)\n",
    "val_x, val_y = zip(*val_pairs)\n",
    "\n",
    "train_x = torch.stack(train_x)\n",
    "train_y = torch.stack(train_y)\n",
    "val_x = torch.stack(val_x)\n",
    "val_y = torch.stack(val_y)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(val_x, val_y)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5695f5",
   "metadata": {},
   "source": [
    "# Definition of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b8a312",
   "metadata": {},
   "source": [
    "The model I used for this task is a transfomer decoder. It's a key component in transformer models for natural language processing. It focuses on generating output sequences by attending to relevant parts of the input sequence. Its architecture allows it to capture dependencies between different words in the input, making it highly effective for tasks like language translation and text generation. Using a transformer decoder enhances the model's ability to understand context and produce coherent and contextually relevant output, making it a powerful choice for various language-related tasks in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee52ecd",
   "metadata": {},
   "source": [
    "![Example Image](transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f840224",
   "metadata": {
    "id": "7f840224"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(0)].detach()\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead)\n",
    "        self.linear1 = nn.Linear(d_model, 2048)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear2 = nn.Linear(2048, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        x = x + self.dropout(attn_output)\n",
    "\n",
    "        ff_output = self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "        x = x + self.dropout(ff_output)\n",
    "\n",
    "        x = self.norm1(x)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len=512):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.transformer_decoder_layers = nn.ModuleList(\n",
    "            [TransformerDecoderLayer(d_model, nhead) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        for layer in self.transformer_decoder_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad877c",
   "metadata": {},
   "source": [
    "**PositionalEncoding:**\n",
    "Adds positional information to input sequences, allowing the model to understand the order of elements in the sequence.\n",
    "\n",
    "**TransformerDecoderLayer:**\n",
    "A single layer of the transformer decoder, consisting of multi-head self-attention, feedforward neural network, and layer normalization. It helps the model capture context and relationships within the input sequence.\n",
    "\n",
    "**TransformerDecoder:**\n",
    "Assembles multiple transformer decoder layers to form a complete decoder. It incorporates embedding, positional encoding, and a final linear layer to output predictions based on the learned representations. This module is the heart of the transformer decoder architecture for language-related tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956791f",
   "metadata": {},
   "source": [
    "# Creation of the instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff9226eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff9226eb",
    "outputId": "fc45abed-5fe1-4ec1-ed2e-af28490529e1"
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size+5\n",
    "d_model = 256 #\n",
    "nhead = 4 #number of head attention layer\n",
    "num_layers = 4 #number of decoder block\n",
    "max_len = 32 #maximum input : the number of token taken in account for predicting the next one\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "decoder = TransformerDecoder(vocab_size, d_model, nhead, num_layers, max_len).to(device)\n",
    "\n",
    "pretrained_camembert = CamembertModel.from_pretrained('camembert-base')\n",
    "pretrained_embedding_weights = pretrained_camembert.embeddings.word_embeddings.weight\n",
    "\n",
    "new_shape = (vocab_size, d_model)\n",
    "reshaped_weights = pretrained_embedding_weights[:vocab_size, :d_model]\n",
    "\n",
    "decoder.embedding.weight = nn.Parameter(reshaped_weights, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba6ad7d",
   "metadata": {},
   "source": [
    "I choosed these parameters empirically. \n",
    "I'm loading part of an embedding layer of a pretrained camembert model compatible with my tokenizer. Loading these weigths slightly the training time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d1e26f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 13485573\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_parameters = count_parameters(decoder)\n",
    "print(f\"Number of parameters in the model: {num_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ZzT1-AOu_uCU",
   "metadata": {
    "id": "ZzT1-AOu_uCU"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=0.0001)\n",
    "\n",
    "print_interval = 100\n",
    "num_epochs = 10\n",
    "\n",
    "decoder.eval()\n",
    "avg_val_loss_list = []\n",
    "loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c559fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    decoder.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = decoder(inputs)\n",
    "\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i%100 == 0:\n",
    "            print(i,f'Loss: {loss:.4f}')\n",
    "        loss_list.append(loss)\n",
    "            \n",
    "    avg_val_loss = 0.0\n",
    "    decoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_i in range(len(val_x)):\n",
    "            val_inputs, val_targets = val_x[val_i], val_y[val_i]\n",
    "            val_outputs = decoder(val_inputs)\n",
    "            val_loss = criterion(val_outputs.view(-1, vocab_size), val_targets.view(-1))\n",
    "            avg_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss /= len(val_x)\n",
    "    avg_val_loss_list.append(avg_val_loss)\n",
    "\n",
    "    print(epoch,f'Avg Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    decoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a1ffce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(decoder.state_dict(), \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd70b64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_state_dict = torch.load(\"model\")\n",
    "decoder = TransformerDecoder(vocab_size, d_model, nhead, num_layers, max_len).to(device)\n",
    "decoder.load_state_dict(decoder_state_dict) #you can load the model just by running this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f450af84",
   "metadata": {},
   "source": [
    "Training 10 epoch took me about 7 hours. The training time will depends on your machine. We can see at the end that the validation loss is increasing again : the model is overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c926be85",
   "metadata": {},
   "source": [
    "![Example Image](graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8635bfd7",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c13a540c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "c13a540c",
    "outputId": "65cab278-0e97-4c8f-c836-724cfe744a4e"
   },
   "outputs": [],
   "source": [
    "def generate_tokens(model, tokenizer, seed_text, n_tokens):\n",
    "    model.eval()\n",
    "\n",
    "    seed_tokens = tokenizer.encode(seed_text, return_tensors=\"pt\")[:,:-1]\n",
    "\n",
    "    for _ in range(n_tokens):\n",
    "        with torch.no_grad():\n",
    "            logits = decoder(seed_tokens)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            seed_tokens = torch.cat([seed_tokens, idx_next], dim=-1)\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    generated_text = tokenizer.decode(seed_tokens.squeeze().tolist())\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb171ba",
   "metadata": {},
   "source": [
    "To generate text, we extract the last token optained and add it to the already generated tokens before putting the sentence back into the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29cf66a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> La fourmie n'est pas préteuse, c'est là son moindre défault, que faisiez-vous au temps chaud? ma bonne est bonne.\n",
      "En en sera parlé :\n",
      "Agré l’aucun rien, sans profondeur ;\n",
      "J’ac bien.\n",
      "Prétendras ;\n",
      "Ne pénétr tous les Roi\n",
      "Si content, dit l'y en peu votre cou fait un marrons : on ne nos bien ; l’homme est un si bien sans plus. É lors plus des sourd autant les proposer l'une leçon ;\n",
      "Sen nous l'y mettraiaient de leurs choux.\n",
      "Fidèle en fit autour de toutes ité de pour l’enfin ne de sens.\n",
      "Indi,\n",
      "Des malheur nos ressorts\n",
      "Et si n cause l'en sans bruit un pesant un moment.\n",
      "Me firent passer homme, pour l'elle ;\n",
      "Mon cousin acco avec des ordres mettre le Chat assez,\n",
      "Jon vous ce récit de ce n'effort.\n",
      "V si haut ;\n",
      "Même grenier dans plus chapitres en souffrant un cent si bien comme la plus tant chéri de changer d'un certain se dés plus\n",
      "La main en pèlerinage d leurs mœurs des Maxime. Souhait, n me reste : je n quoi?\n",
      "Et ne vint en mal.\n",
      "Qu’est-hu\n",
      "Lors lui prête un arbre, gare, l'yon.\n",
      "Ven ce n parent de condamner\n",
      "Tu ne se blottiffre des tiens par cet <unk> combien de tout le l’un plus, il y quelque herbe, des vides de tout venant.\n",
      "S, bien d’<unk>ne, tombant de la vengé l'ont l'irai pas d’herbe votre trépas.\n",
      "All qui du maître.\n",
      "Mél.\n",
      "Le vent, à tout dit bien, le Loups\n",
      "S irait par louï cette Déesse\n",
      "Un sergent.\n",
      "Que les esprits.\n",
      "Ma l'en viens des sa grottes :\n",
      "Les gens-envoy, certains endroits,\n",
      "Car quel est confus des merveilles et tant,\n",
      "Un vieux,\n",
      "L'on Hévert mo fer par son, si bien fait le droit à sa complaisance.\n",
      "Tout ce un plat.\n",
      "Tout envers nous n’He ce par mille ans douée.\n",
      "Cette perd tout ne point, cru.\n",
      "Tours en ses gouverneur.\n",
      "Quand des belles\n",
      "Un jour se loger sous les exigé. fort faire ;\n",
      "Le vase en propos.\n",
      "Pour savoir,\n",
      "Le pouvoir goût de courage à l'yons- qu'exemple tombéêtre atteint\n"
     ]
    }
   ],
   "source": [
    "model = decoder\n",
    "\n",
    "seed_text = \"La fourmie n'est pas préteuse, c'est là son moindre défault\"\n",
    "n_tokens = 500\n",
    "\n",
    "generated_text = generate_tokens(model, tokenizer, seed_text, n_tokens)\n",
    "print(generated_text.replace('@','\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe67701",
   "metadata": {},
   "source": [
    "The generated output exhibits limited coherence, unsurprising given the modest size of the model, the constraints of the database, and the brief training duration. However, we can see that the model seems to understand that a sentence starts with capital letters and that certain sequences of 3 to 4 words are correct. These findings, though preliminary, provide insights into the model's potential capabilities with further refinement."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
